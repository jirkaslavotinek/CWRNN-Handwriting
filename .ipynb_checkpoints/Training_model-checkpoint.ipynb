{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating handwriting with Clockwork-RNN\n",
    "This is the implementation of handwriting generation with Clockwork-RNN(CWRNN) in Mixture Density Network(MDN).\n",
    "\n",
    "The main idea behind MDN is to use neural network outputs to parameterise a mixture distribution.\n",
    "\n",
    "The idea behind CWRNN is to use more RNN cells but not to use all of them in every timestep. Cells in CWRNN have periods which tell us in what timestep the cell should be active.\n",
    "\n",
    "This implemetation is heavily influenced by these projects:\n",
    "*  https://github.com/aidangomez/models/tree/684883e4d4f310e59da109ba28cda8ba5f7785c9/clockwork_rnn\n",
    "*  https://github.com/hardmaru/write-rnn-tensorflow\n",
    "\n",
    "And these papers:\n",
    "*  https://arxiv.org/pdf/1308.0850.pdf\n",
    "*  https://arxiv.org/pdf/1402.3511.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from xml files. Data can be found at http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database. I only used 300 files of total 12000 for time convenience. Path to data is saved in var $path$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "path = \"C:/Users/Jirka/Downloads/lineStrokes\"\n",
    "filenames = []\n",
    "for dir1 in os.listdir(path):\n",
    "    for dir2 in os.listdir(path+\"/\"+dir1):\n",
    "        for filename in os.listdir(path+\"/\"+dir1+\"/\"+dir2):\n",
    "            filenames.append(path+\"/\"+dir1+\"/\"+dir2+\"/\"+filename)\n",
    "            \n",
    "trainSequences = []\n",
    "            \n",
    "for filename in filenames:  \n",
    "    root = ET.parse(filename).getroot();\n",
    "    XY = root.find('WhiteboardDescription')\n",
    "\n",
    "    maxX = (float)(XY.find('DiagonallyOppositeCoords').get('x'))\n",
    "    maxY = (float)(XY.find('DiagonallyOppositeCoords').get('y'))\n",
    "    minX = (float)(XY.find('VerticallyOppositeCoords').get('x'))\n",
    "    minY = (float)(XY.find('HorizontallyOppositeCoords').get('y'))\n",
    "\n",
    "    line = []\n",
    "    for stroke in root.iter('Stroke'):\n",
    "        sTime = (float)(stroke.get('start_time'))\n",
    "        eTime = (float)(stroke.get('end_time'))\n",
    "        dTime = ((eTime-sTime) if (sTime-eTime)>0 else 1)\n",
    "        for point in stroke.iter('Point'):\n",
    "            line.append([int(point.get('x')), int(point.get('y')), 0]);\n",
    "        line[-1][2] = 1\n",
    "    \n",
    "    for it in range(len(line)-1, 0, -1):\n",
    "        line[it] = [line[it][0]-line[it-1][0],\n",
    "                    line[it][1]-line[it-1][1],\n",
    "                    line[it][2]]\n",
    "    line[0] = [0, 0, 0]\n",
    "    trainSequences.append(line)\n",
    "\n",
    "npTrainSequences = []\n",
    "for line in trainSequences:\n",
    "    npTrainSequences.append(np.array(line, dtype=np.int16))\n",
    "    \n",
    "    \n",
    "valid_data = []\n",
    "train_data = []\n",
    "counter = 0\n",
    "cur_data_counter = 0\n",
    "\n",
    "for data in npTrainSequences:\n",
    "    if len(data) > (300+2):\n",
    "        # removes large gaps from the data\n",
    "        data = np.minimum(data, 500)\n",
    "        data = np.maximum(data, -500)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        data[:,0:2] /= 10\n",
    "        cur_data_counter = cur_data_counter + 1\n",
    "        if cur_data_counter % 20 == 0:\n",
    "            valid_data.append(data)\n",
    "        else:\n",
    "            train_data.append(data)\n",
    "            counter += int(len(data)/((300+2))) # number of equiv batches this datapoint is worth\n",
    "            \n",
    "print(\"train data: {}, valid data: {}\".format(len(train_data), len(valid_data)))\n",
    "    \n",
    "size = 0\n",
    "for a in npTrainSequences:\n",
    "    size += sys.getsizeof(a)\n",
    "\n",
    "#Data loader prepared for batch fetching to neural network.\n",
    "class DataLoader():\n",
    "    def __init__(self, train_data, valid_data, batch_size=50):\n",
    "        self.seq_length = 300\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.batch_size = batch_size\n",
    "        self.reset_batch_pointer()\n",
    "        self.num_batches = int(counter / self.batch_size)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        # returns a randomised, seq_length sized portion of the training data\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            data = self.train_data[self.pointer]\n",
    "            n_batch = int(len(data)/((self.seq_length+2))) # number of equiv batches this datapoint is worth\n",
    "            idx = random.randint(0, len(data)-self.seq_length-2)\n",
    "            x_batch.append(np.copy(data[idx:idx+self.seq_length]))\n",
    "            y_batch.append(np.copy(data[idx+1:idx+self.seq_length+1]))\n",
    "            if random.random() < (1.0/float(n_batch)): # adjust sampling probability.\n",
    "                #if this is a long datapoint, sample this data more with higher probability\n",
    "                self.pointer += 1\n",
    "                if (self.pointer >= len(self.train_data)):\n",
    "                    self.pointer = 0\n",
    "        return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from clockwork.cwrnn import CWRNNCell\n",
    "\n",
    "batch_size = 50\n",
    "seq_len = 300\n",
    "num_components = 3\n",
    "\n",
    "NHIDDEN = 128*8\n",
    "NCELLS = 8\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_len, num_components])\n",
    "targets = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_len, num_components])\n",
    "\n",
    "#CWRNN Part\n",
    "cells = []\n",
    "for i in range(NCELLS):\n",
    "    cells.append(tf.contrib.rnn.BasicRNNCell(num_units=NHIDDEN//NCELLS))\n",
    "\n",
    "periods = []\n",
    "for i in range(NCELLS):\n",
    "    periods.append(2**i)\n",
    "\n",
    "cell = CWRNNCell(cells, periods, state_is_tuple=False)\n",
    "\n",
    "#LSTM Part\n",
    "#cell = tf.contrib.rnn.BasicLSTMCell(NHIDDEN, state_is_tuple=False)\n",
    "\n",
    "#cell = tf.contrib.rnn.MultiRNNCell(\n",
    "#            [tf.contrib.rnn.BasicLSTMCell(NHIDDEN, state_is_tuple=False) for _ in range(2)],\n",
    "#            state_is_tuple=False\n",
    "#)\n",
    "\n",
    "#cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = 0.8)\n",
    "\n",
    "zero_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "prev_state = tf.identity(zero_state, name='prev_state')\n",
    "\n",
    "NOUT = 1 + 20 * 6 # end_of_stroke + prob + 2*(mu + sig) + corr\n",
    "\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    output_w = tf.get_variable(\"output_w\", [NHIDDEN, NOUT])\n",
    "    output_b = tf.get_variable(\"output_b\", [NOUT])\n",
    "    \n",
    "inputs2 = tf.unstack(inputs, axis=1)\n",
    "\n",
    "#Forward pass\n",
    "outputs, state_out = tf.contrib.legacy_seq2seq.rnn_decoder(inputs2, prev_state, cell, loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, NHIDDEN])\n",
    "output = tf.nn.xw_plus_b(output, output_w, output_b)\n",
    "state_out = tf.identity(state_out, name='state_out')\n",
    "\n",
    "#Some trying\n",
    "\n",
    "#outputs, _ = tf.contrib.legacy_seq2seq.rnn_decoder(cell=cell, decoder_inputs=x, initial_state=zero_state)\n",
    "#outputs, out_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=prev_state, time_major=False)\n",
    "\n",
    "#NOUT = 1 + 20 * 6 # end_of_stroke + prob + 2*(mu + sig) + corr\n",
    "\n",
    "#W_out = tf.Variable(tf.random_normal([NHIDDEN,NOUT], stddev=1.0, dtype=tf.float32))\n",
    "#b_out = tf.Variable(tf.random_normal([NOUT], stddev=1.0, dtype=tf.float32))\n",
    "\n",
    "#output = tf.reshape(outputs, [-1, NHIDDEN])\n",
    "#output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, NHIDDEN])\n",
    "#print(output.shape)\n",
    "#output = tf.nn.xw_plus_b(output, W_out, b_out)\n",
    "\n",
    "#End of some trying\n",
    "\n",
    "\n",
    "flat_targets = tf.reshape(targets, [-1, num_components])\n",
    "[x1_data, x2_data, eos_data] = tf.split(axis=1, num_or_size_splits=3, value=flat_targets)\n",
    "\n",
    "#MDN Part. Mainly copied from https://github.com/hardmaru/write-rnn-tensorflow.\n",
    "def tf_2d_normal(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    # eq # 24 and 25 of http://arxiv.org/abs/1308.0850\n",
    "    norm1 = tf.subtract(x1, mu1)\n",
    "    norm2 = tf.subtract(x2, mu2)\n",
    "    s1s2 = tf.multiply(s1, s2)\n",
    "    z = tf.square(tf.div(norm1, s1))+tf.square(tf.div(norm2, s2))-2*tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "    negRho = 1-tf.square(rho)\n",
    "    result = tf.exp(tf.div(-z,2*negRho))\n",
    "    denom = 2*np.pi*tf.multiply(s1s2, tf.sqrt(negRho))\n",
    "    result = tf.div(result, denom)\n",
    "    return result\n",
    "\n",
    "def get_lossfunc(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos, x1_data, x2_data, eos_data):\n",
    "    result0 = tf_2d_normal(x1_data, x2_data, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr)\n",
    "    # implementing eq # 26 of http://arxiv.org/abs/1308.0850\n",
    "    epsilon = 1e-20\n",
    "    result1 = tf.multiply(result0, z_pi)\n",
    "    result1 = tf.reduce_sum(result1, 1, keep_dims=True)\n",
    "    result1 = -tf.log(tf.maximum(result1, 1e-20)) # at the beginning, some errors are exactly zero.\n",
    "\n",
    "    result2 = tf.multiply(z_eos, eos_data) + tf.multiply(1-z_eos, 1-eos_data)\n",
    "    result2 = -tf.log(result2)\n",
    "\n",
    "    result = result1 + result2\n",
    "    return tf.reduce_sum(result)\n",
    "\n",
    "# below is where we need to do MDN splitting of distribution params\n",
    "def get_mixture_coef(output):\n",
    "    # returns the tf slices containing mdn dist params\n",
    "    # ie, eq 18 -> 23 of http://arxiv.org/abs/1308.0850\n",
    "    z = output\n",
    "    z_eos = z[:, 0:1]\n",
    "    z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = tf.split(axis=1, num_or_size_splits=6, value=z[:, 1:])\n",
    "\n",
    "    # process output z's into MDN paramters\n",
    "\n",
    "    # end of stroke signal\n",
    "    z_eos = tf.sigmoid(z_eos) # should be negated, but doesn't matter.\n",
    "\n",
    "    # softmax all the pi's:\n",
    "    max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "    z_pi = tf.subtract(z_pi, max_pi)\n",
    "    z_pi = tf.exp(z_pi)\n",
    "    normalize_pi = tf.reciprocal(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "    z_pi = tf.multiply(normalize_pi, z_pi)\n",
    "\n",
    "    # exponentiate the sigmas and also make corr between -1 and 1.\n",
    "    z_sigma1 = tf.exp(z_sigma1)\n",
    "    z_sigma2 = tf.exp(z_sigma2)\n",
    "    z_corr = tf.tanh(z_corr)\n",
    "\n",
    "    return [z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos]\n",
    "\n",
    "#Get distribution parameters from network output\n",
    "[o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos] = get_mixture_coef(output)\n",
    "\n",
    "#Parameters for outputs\n",
    "data_out_pi = tf.identity(o_pi, \"data_out_pi\");\n",
    "data_out_mu1 = tf.identity(o_mu1, \"data_out_mu1\");\n",
    "data_out_mu2 = tf.identity(o_mu2, \"data_out_mu2\");\n",
    "data_out_sigma1 = tf.identity(o_sigma1, \"data_out_sigma1\");\n",
    "data_out_sigma2 = tf.identity(o_sigma2, \"data_out_sigma2\");\n",
    "data_out_corr = tf.identity(o_corr, \"data_out_corr\");\n",
    "data_out_eos = tf.identity(o_eos, \"data_out_eos\");\n",
    "\n",
    "#Loss Function\n",
    "lossfunc = get_lossfunc(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos, x1_data, x2_data, eos_data)\n",
    "cost = lossfunc / (batch_size * seq_len)\n",
    "\n",
    "#Train network with AdamOptimizer and desired learninig rate with gradient clippingg\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 10.0)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\Jirka\\\\Desktop\\\\CWRNN_LONG\\\\\"\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "data_loader = DataLoader(train_data, valid_data, batch_size)\n",
    "NEPOCH = 40\n",
    "for e in range(NEPOCH):\n",
    "    sess.run(tf.assign(lr, 0.005 * (0.95 ** e)))\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    state = prev_state.eval()\n",
    "    for b in range(data_loader.num_batches):\n",
    "        i = e * data_loader.num_batches + b\n",
    "        start = time.time()\n",
    "        x, y = data_loader.next_batch()\n",
    "        feed = {inputs: x, targets: y, prev_state: state}\n",
    "        train_loss, state, _ = sess.run([cost, state_out, train_op], feed) \n",
    "\n",
    "        end = time.time()\n",
    "        print(\n",
    "            \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"  \\\n",
    "            .format(\n",
    "                i,\n",
    "                NEPOCH * data_loader.num_batches,\n",
    "                e, \n",
    "                train_loss, end - start))\n",
    "        if (e * data_loader.num_batches + b) % 100 == 0 and ((e * data_loader.num_batches + b) > 0):\n",
    "            checkpoint_path = os.path.join(model_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n",
    "            print(\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
